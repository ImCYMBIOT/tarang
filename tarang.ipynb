{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ea89245a",
   "metadata": {},
   "source": [
    "IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ce7a47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "# import tensorflow as tf\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c116b0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.layers import Flatten, Dropout, Activation\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebe2b624",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the base directory where the user-specific folders are located\n",
    "base_directory = r\"D:/github/tarang/data\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "523ed567",
   "metadata": {},
   "source": [
    " Get a list of all user-specific folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bb650b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_folders = [folder for folder in os.listdir(base_directory) if folder.startswith(\"my_spotify_data\")]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f66dff1",
   "metadata": {},
   "source": [
    " Initialize lists to store the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e34b9a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_data = []\n",
    "streaming_data = []\n",
    "user_data = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36cecd47",
   "metadata": {},
   "source": [
    " Iterate over the user folders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d27f894",
   "metadata": {},
   "outputs": [],
   "source": [
    "for user_folder in user_folders:\n",
    "    # Construct the path to the JSON files within each user folder\n",
    "    search_json_path = os.path.join(base_directory, user_folder, \"MyData\", \"SearchQueries.json\")\n",
    "    streaming_json_path = os.path.join(base_directory, user_folder, \"MyData\", \"StreamingHistory0.json\")\n",
    "    user_json_path = os.path.join(base_directory, user_folder, \"MyData\", \"UserData.json\")\n",
    "\n",
    "    # Read and process the search history JSON file\n",
    "    with open(search_json_path, \"r\") as file:\n",
    "        search_json_data = json.load(file)\n",
    "        for item in search_json_data:\n",
    "            search_data.append({\n",
    "                \"searchTime\": item[\"searchTime\"],\n",
    "                \"searchQuery\": item[\"searchQuery\"]\n",
    "            })\n",
    "\n",
    "    # Read and process the streaming history JSON file\n",
    "    with open(streaming_json_path, \"r\" , encoding = \"utf-8\") as file:\n",
    "        streaming_json_data = json.load(file)\n",
    "        for item in streaming_json_data:\n",
    "            streaming_data.append({\n",
    "                \"endTime\": item[\"endTime\"],\n",
    "                \"artistName\": item[\"artistName\"],\n",
    "                \"trackName\": item[\"trackName\"],\n",
    "                \"msPlayed\": item[\"msPlayed\"]\n",
    "            })\n",
    "\n",
    "    # Read and process the user data JSON file\n",
    "    with open(user_json_path, \"r\") as file:\n",
    "        user_json_data = json.load(file)\n",
    "        user_data.append({\n",
    "            \"username\": user_json_data[\"username\"],\n",
    "            \"email\": user_json_data[\"email\"],\n",
    "            \"country\": user_json_data[\"country\"],\n",
    "            \"birthdate\": user_json_data[\"birthdate\"],\n",
    "            \"gender\": user_json_data[\"gender\"]\n",
    "        })"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "270c7c83",
   "metadata": {},
   "source": [
    "Convert the search history data to a pandas DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1ee296d",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_data = pd.DataFrame(search_data)\n",
    "search_data['searchTime'] = pd.to_datetime(search_data['searchTime'], format='%Y-%m-%dT%H:%M:%S.%fZ', errors='coerce')\n",
    "search_data['hour_of_day'] = search_data['searchTime'].dt.hour\n",
    "search_data['day_of_week'] = search_data['searchTime'].dt.dayofweek\n",
    "search_data['query_length'] = search_data['searchQuery'].apply(lambda x: len(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a6317f2",
   "metadata": {},
   "source": [
    "Convert the streaming history data to a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fab96e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_data = pd.DataFrame(streaming_data)\n",
    "streaming_data['endTime'] = pd.to_datetime(streaming_data['endTime'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "644d2873",
   "metadata": {},
   "source": [
    "Convert the user data to a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "635a1997",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data = pd.DataFrame(user_data)\n",
    "# user_data['birthdate'] = pd.to_datetime(user_data['birthdate'])\n",
    "# user_data['age'] = (pd.Timestamp.now() - user_data['birthdate']).astype('<m8[Y]')\n",
    "user_data['gender'] = LabelEncoder().fit_transform(user_data['gender'])\n",
    "\n",
    "user_data['birthdate'] = pd.to_datetime(user_data['birthdate'])\n",
    "user_data['age'] = user_data['birthdate'].apply(lambda x: relativedelta(pd.Timestamp.now(), x).years)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e200a403",
   "metadata": {},
   "source": [
    "Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd6bf925",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = pd.merge(search_data, streaming_data, how='outer', left_index=True, right_index=True)\n",
    "merged_data = pd.merge(merged_data, user_data, how='inner', left_index=True, right_index=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e293cd2",
   "metadata": {},
   "source": [
    "Encode categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ad08a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data['country'] = LabelEncoder().fit_transform(merged_data['country'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "313eb4ec",
   "metadata": {},
   "source": [
    "Normalize numerical features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "976019f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "merged_data[['msPlayed']] = scaler.fit_transform(merged_data[['msPlayed']])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f8a2bed4",
   "metadata": {},
   "source": [
    "Split data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2106ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(merged_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50281197",
   "metadata": {},
   "source": [
    "Prepare inputs for LSTM architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a655c36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_inputs = train_data[['hour_of_day', 'day_of_week', 'query_length']]\n",
    "search_inputs = np.asarray(search_inputs)\n",
    "search_inputs = np.expand_dims(search_inputs, axis=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f6c9faa",
   "metadata": {},
   "source": [
    "Prepare inputs for collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "400378a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_inputs = train_data[['age', 'gender', 'country']]\n",
    "user_inputs = np.asarray(user_inputs)\n",
    "user_inputs = to_categorical(user_inputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f047860",
   "metadata": {},
   "source": [
    "Prepare targets for collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18508352",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_inputs = train_data[['msPlayed']]\n",
    "target_inputs = np.asarray(target_inputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dce488ce",
   "metadata": {},
   "source": [
    "LSTM architecture for search history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "078f7dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_input = Input(shape=(3, 1))\n",
    "lstm_output = LSTM(64)(lstm_input)\n",
    "lstm_output = Dense(32, activation='relu')(lstm_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe614d6a",
   "metadata": {},
   "source": [
    "Autoencoder architecture for collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88f75f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = Input(shape=(user_inputs.shape[1],))\n",
    "user_embedding = Dense(64, activation='relu')(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3151d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_input = Input(shape=(target_inputs.shape[1],))\n",
    "target_embedding = Dense(64, activation='relu')(target_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7eb5870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_layer = Concatenate()([user_embedding, target_embedding])\n",
    "encoded = Dense(32, activation='relu', activity_regularizer=regularizers.l1(10e-5))(merged_layer)\n",
    "decoded = Dense(64, activation='relu')(encoded)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0635a6c0",
   "metadata": {},
   "source": [
    "Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36cf0cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Model(inputs=[user_input, target_input], outputs=decoded)\n",
    "autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss=MeanSquaredError())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d74d8123",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7dbe2a99",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Training data contains 1 samples, which is not sufficient to split it into a validation and training set as specified by `validation_split=0.2`. Either provide more data, or a different value for the `validation_split` argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m autoencoder\u001b[39m.\u001b[39;49mfit([user_inputs, target_inputs], target_inputs, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49m[EarlyStopping(patience\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)])\n",
      "File \u001b[1;32md:\\github\\tarang\\.venv\\Lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32md:\\github\\tarang\\.venv\\Lib\\site-packages\\keras\\engine\\data_adapter.py:1687\u001b[0m, in \u001b[0;36mtrain_validation_split\u001b[1;34m(arrays, validation_split)\u001b[0m\n\u001b[0;32m   1684\u001b[0m split_at \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(math\u001b[39m.\u001b[39mfloor(batch_dim \u001b[39m*\u001b[39m (\u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m validation_split)))\n\u001b[0;32m   1686\u001b[0m \u001b[39mif\u001b[39;00m split_at \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m split_at \u001b[39m==\u001b[39m batch_dim:\n\u001b[1;32m-> 1687\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1688\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTraining data contains \u001b[39m\u001b[39m{batch_dim}\u001b[39;00m\u001b[39m samples, which is not \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1689\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msufficient to split it into a validation and training set as \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1690\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mspecified by `validation_split=\u001b[39m\u001b[39m{validation_split}\u001b[39;00m\u001b[39m`. Either \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1691\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mprovide more data, or a different value for the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1692\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`validation_split` argument.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1693\u001b[0m             batch_dim\u001b[39m=\u001b[39mbatch_dim, validation_split\u001b[39m=\u001b[39mvalidation_split\n\u001b[0;32m   1694\u001b[0m         )\n\u001b[0;32m   1695\u001b[0m     )\n\u001b[0;32m   1697\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_split\u001b[39m(t, start, end):\n\u001b[0;32m   1698\u001b[0m     \u001b[39mif\u001b[39;00m t \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Training data contains 1 samples, which is not sufficient to split it into a validation and training set as specified by `validation_split=0.2`. Either provide more data, or a different value for the `validation_split` argument."
     ]
    }
   ],
   "source": [
    "autoencoder.fit([user_inputs, target_inputs], target_inputs, epochs=10, batch_size=32, validation_split=0.2, callbacks=[EarlyStopping(patience=3)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa1854d4",
   "metadata": {},
   "source": [
    "Extract embeddings from the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df634e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = Model(inputs=user_input, outputs=user_embedding)\n",
    "user_embeddings = embedding_model.predict(user_inputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e82ee3a4",
   "metadata": {},
   "source": [
    "Generate recommendations based on user preferences\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
